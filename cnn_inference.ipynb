{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "cnn_inference.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patbaa/demo_notebooks/blob/master/cnn_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf6ERITRhnh5",
        "colab_type": "text"
      },
      "source": [
        "# CNN inference notebook\n",
        "\n",
        "ImageNet consists of more than 1 million images separated into 1000 different categories. ImageNet is the _default_ dataset for computer vision (especially classification) banchmarks in 2010s.\n",
        "Most of the state-of-the-art models are trained on the ImageNet data to prove their effectiveness. Luckily these models are available in tensorflow along their trained weights.\n",
        "\n",
        "So to try out a trained convolutional neural network we can download these weights, we do not need to train the models for hours/days.\n",
        "\n",
        "In this notebook we will use VGG16."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9MZRoifho9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJWbhfEdhnh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications.vgg16 import *\n",
        "%pylab inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu2Bx17rhniB",
        "colab_type": "text"
      },
      "source": [
        "### Loading VGG16 model\n",
        " - include top $\\to$ if include the fully connected layers at the end\n",
        " - weights $\\to$ we want to use the weights that were trained via the ImageNet dataset\n",
        " - input_tensor, input_shape $\\to$ optinal. If top is True, then  it must be (224, 224, 3)\n",
        " - pooling $\\to$ if top is False then we can set the last pooling layer\n",
        " - classes $\\to$ when top is True and weights is None we can set #classes\n",
        " \n",
        "**In modern CNNs the last pooling layer is often global maxpooling, so the result of that layer is (1x1xC) regardless the input image dimension.**\n",
        " \n",
        "For the first time running the cell below the weights are downloaded, later they are fetched from a local file!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKc3ntughniC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = VGG16(include_top=True,\n",
        "              weights='imagenet',\n",
        "              input_tensor=None,\n",
        "              input_shape=None,\n",
        "              pooling=None,\n",
        "              classes=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0cz3FWphniG",
        "colab_type": "text"
      },
      "source": [
        "### Shape: (batch_size, x, y, color_channel)\n",
        "Also you can use channel_first format is needed (batch_size, color_channel, x, y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clI6j2XjhniH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjVtbppzhniK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://scx1.b-cdn.net/csz/news/800/2018/2-dog.jpg -O dog.jpg -q    \n",
        "# Credit: CC0 Public Domain\n",
        "pil_dog = Image.open('dog.jpg')\n",
        "cv2_dog = cv2.imread('dog.jpg')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(121)\n",
        "plt.imshow(pil_dog)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(cv2_dog)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7yKZM7GhniO",
        "colab_type": "text"
      },
      "source": [
        "# Heh!?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pj8eDuShniQ",
        "colab_type": "text"
      },
      "source": [
        "## Python and images\n",
        "\n",
        "There are two main libararies in Python to handle images, namely the PIL (pillow) and CV2 (opencv). Both are fine, but remember:\n",
        " - **PIL** uses **RGB** as color channel order\n",
        " - **cv2** uses **BGR** as color channel order\n",
        " \n",
        "This can lead to significant confusion! One might train the model using cv2 for loading the image, the customer may use the model with PIL loaded images and the performance will be different! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umt8QVYEiprA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(121)\n",
        "plt.imshow(pil_dog)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(cv2_dog[...,::-1]) # revert color order\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqbk8fvbhnie",
        "colab_type": "text"
      },
      "source": [
        "## Image preprocessing\n",
        "\n",
        "Usually images are preprocessed before feeding them to a neural network. We have seen before that scaling the data helps for many machine learning models. It is similar to convolutional neural networks. There are many ways to preprocess the data including:\n",
        " - color channel ordering\n",
        " - mean modification\n",
        " - std modification\n",
        " \n",
        "If we want to use a pre-trained model and keep its accuracy we must preprocess our images the same way as the training images were preprocessed.\n",
        "\n",
        "For VGG16 in tf.keras the following preprocessing were done during training:\n",
        " - mean removal\n",
        " - color channel order is BGR\n",
        " \n",
        "The BGR means are 103.939, 116.779, 123.68, which were calculated from the ImageNet training data. For these information one needs to read the API (in lucky cases) or check the source code.  \n",
        "Also, often preprocess_input functions are provided, but be careful, it is not possible to retreive color channel ordering from a 3D numpy array!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-xLd5RIhnif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(img, RGB=True):\n",
        "    '''\n",
        "    For RGB images (PIL) use RGB=True\n",
        "    For BGR images (cv2) use RGB=False\n",
        "    '''\n",
        "    img  = np.array(img.resize((224, 224))).astype(float)\n",
        "    if RGB:\n",
        "        img = img[...,::-1]\n",
        "    img -= [103.939, 116.779, 123.68]\n",
        "    \n",
        "    return img[None, ...] # to form a batch with batch_size of 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6KTiE2hhnij",
        "colab_type": "text"
      },
      "source": [
        "# Generating the predictions\n",
        " - we need 224x224 pixel images\n",
        "   - crop\n",
        "   - resize\n",
        " - the predictions are a 1000 long vectors\n",
        " - the provided decode_predictions function shows the top predictions in human readable format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jGnDbaShnik",
        "colab_type": "text"
      },
      "source": [
        "## Resize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC2diqlIhnil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pil_dog.resize((224, 224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO4Mvye6hnio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(preprocess(pil_dog))\n",
        "preds[0][:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "neQVrcClhnis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(preprocess(pil_dog))\n",
        "decode_predictions(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr-RmQMLhniw",
        "colab_type": "text"
      },
      "source": [
        "Wrong color channel order:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bYICgMKqhnix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(preprocess(pil_dog, RGB=False))\n",
        "decode_predictions(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh6iD_8Phni0",
        "colab_type": "text"
      },
      "source": [
        "## Crop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9zjrnSzhni1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dx = 80\n",
        "dy = 0\n",
        "img2 = pil_dog.resize((int(pil_dog.size[0]*0.5), int(pil_dog.size[1]*0.5))).crop((dx, dy, dx+224, dy+224))\n",
        "img2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aTAGVUUhni6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(preprocess(img2))\n",
        "decode_predictions(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jsbaDWthni-",
        "colab_type": "text"
      },
      "source": [
        "# Flipping images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMwLkilUhnjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pil_dog.transpose(Image.FLIP_LEFT_RIGHT).resize((224, 224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "jUrG129rhnjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(preprocess(pil_dog.transpose(Image.FLIP_LEFT_RIGHT)))\n",
        "decode_predictions(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN1EaMZbhnjH",
        "colab_type": "text"
      },
      "source": [
        "The model did not see top-bottom flipped images during training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A34qjhu5hnjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pil_dog.transpose(Image.FLIP_TOP_BOTTOM).resize((224, 224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Qv3N5TEahnjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(preprocess(pil_dog.transpose(Image.FLIP_TOP_BOTTOM)))\n",
        "decode_predictions(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFhwXu2DhnjP",
        "colab_type": "text"
      },
      "source": [
        "Upside down & wrong color channel order $\\to$ pretty bad "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkodtbTPhnjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(np.array(pil_dog.transpose(Image.FLIP_TOP_BOTTOM).resize((224, 224)))[...,::-1])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LsRRrcg4hnjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(preprocess(pil_dog.transpose(Image.FLIP_TOP_BOTTOM), RGB=False))\n",
        "decode_predictions(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOpuKg0whnjX",
        "colab_type": "text"
      },
      "source": [
        "# Other examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er0AtE7mhnjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q -O eagle.jpg https://3.bp.blogspot.com/-ZgrHOoWo8Bs/WFCLpESwZ8I/AAAAAAAAT64/h25mc-NsUPA6qqLR0PqX1xN3DqA_M-PCQCEw/s1600/DSC_0452.JPG\n",
        "# credits to https://seasonsinthevalley.blogspot.com/\n",
        "eagle = Image.open('eagle.jpg')\n",
        "eagle.resize((600, 400))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgRpw9bphnja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c8952e8a-17cc-4768-acb2-a9be3273e4bb"
      },
      "source": [
        "preds = model.predict(preprocess(eagle))\n",
        "decode_predictions(preds)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('n01616318', 'vulture', 0.41468802),\n",
              "  ('n01614925', 'bald_eagle', 0.40434498),\n",
              "  ('n01608432', 'kite', 0.095860355),\n",
              "  ('n01582220', 'magpie', 0.017725317),\n",
              "  ('n01829413', 'hornbill', 0.01550091)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yc6qtSphnje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eagle = eagle.crop((500, 250, 800, 550))\n",
        "eagle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNMcTdxAhnji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(preprocess(eagle))\n",
        "decode_predictions(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0aTYO0Mhnjl",
        "colab_type": "text"
      },
      "source": [
        "With a proper cropping the **bald_eagle probability significantly improved**! Further possibility: multi-crop images and keep the average of the predictions / the most probable tile's prediction etc..  \n",
        "Cropping an image with a too small object does not make sene (99 crop background, 1 crop object).   \n",
        "Also cropping a good and large image does not makes not much sense (individual images gets too little information: eyeball, ear, ..etc)"
      ]
    }
  ]
}