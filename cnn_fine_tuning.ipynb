{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "cnn_fine_tuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patbaa/demo_notebooks/blob/master/cnn_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7hV6DXVX5DW",
        "colab_type": "text"
      },
      "source": [
        "# CNN fine tuning\n",
        "\n",
        "We have seen that for the ImageNet dataset convolutional neural networks can work surprisingly well. The ImageNet dataset consists of more than 1 million training image. Until now we stated that the deep learning revolution could happen due to the increasing computational power and the enormous data availability.   \n",
        "**What if we do not have too many images?**\n",
        " 1. go and collect much more data\n",
        " 2. use the knowledge learned from the ImageNet dataset\n",
        " \n",
        "Using a neural network that was trained on 1 million ImageNet images must have a **good inner representation** (finding round objects, eyes, text etc.) for photos of various objects. The idea of transfer learning is that we rely on these representations. Unfortunately we **cannot use the pre-trained model as it is**, because we have different categories than the ImageNet, but as we have real-world (ImageNet-like) images we can use all the pre-trained CNN weights except for the last prediction layer.\n",
        "\n",
        "So we have to change the last dense layer of 1000 neurons to match our problem. We can whether train all the weight in the neural network for a small time or we can freze all the weights except for the new last layer and train only that layer. It is also common that one trains only the last layer and when the weights converged they unfreze the rest of the weights to train all of them for a short time.\n",
        "\n",
        "Sometimes the learning rate changes from layer to layer. We will see later that CNNs first layers are usually captures general properties of images (round object or parallel lines) and the last layers are more task specific (dog eye, human head etc.). In general during transfer learning we want to keep as much information one dataset to the other as possible. As the first layers are generic they do not need to change much, so a much lower learning rate can be fine. The last one are more task specific so we have to train them more (with higher learning rate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnRGqh3iYHQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOHfVCQlX5DZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq2rEw2oX5De",
        "colab_type": "text"
      },
      "source": [
        "# Dogs and cats\n",
        "\n",
        "We will use data from the Kaggle challenge 'Dogs and cats'. To make things more difficult we restrict ourselves to a tiny subset of the training images, 300 images for dogs and 300 for cats.\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/3362/media/woof_meow.jpg\">\n",
        "\n",
        "https://www.kaggle.com/c/dogs-vs-cats/\n",
        "\n",
        "ImageNet pre-trained models seem to be a perfect choice as ImageNet contains 100+ different dog breeds, so the network weights should be exceptional when it comes to dog classification.\n",
        "\n",
        "**You can download and unzip the data via uncommenting the two cells below and running them!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jv_tHkLX5Dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download the prepared dataset\n",
        "#!wget http://patbaa.web.elte.hu/dogscats.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpc8H1hpX5Dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unzip it\n",
        "#!unzip -q dogscats.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8BxqIpYX5Dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dogs = list(Path('train/dog/').glob('*'))\n",
        "train_cats = list(Path('train/cat/').glob('*'))\n",
        "\n",
        "print(f'#Training images: {len(train_cats), len(train_dogs)}')\n",
        "\n",
        "test_dogs = list(Path('test/dog/').glob('*'))\n",
        "test_cats = list(Path('test/cat/').glob('*'))\n",
        "\n",
        "print(f'#Test images: {len(test_cats), len(test_dogs)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ojQEtIZ8B5",
        "colab_type": "text"
      },
      "source": [
        "### Most of the images are proper, but a few ones are tricky\n",
        "looking only at the training images, we expect similar in the test ones too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdSw-3dVZ579",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open('train/cat/cat.11100.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnqivYyxZ6QZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open('train/cat/cat.11368.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qXntVQ9Z6i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open('train/cat/cat.11184.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQBNM9aBZ64f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open('train/dog/dog.11125.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcPbsZ4oZ7IZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open('train/dog/dog.11350.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcAzYcEtZ7b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open('train/dog/dog.11191.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__X35x65aG9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open('train/dog/dog.11299.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTKqS_h2X5Du",
        "colab_type": "text"
      },
      "source": [
        "## Setting up the models\n",
        "\n",
        "We will use ResNet50 (will learn about it later). We use two models:\n",
        " 1. randomly initialized\n",
        " 2. pre-trained on imagenet\n",
        " \n",
        "As ImageNet contains 1000 categories we have to change tha last layer to have 1 neuron instead of 1000. We will train a binary classifier (0-1) to indicate if we have a cat or a dog on the image. \n",
        " - binary crossentropy\n",
        " - sigmoid instead of softmax\n",
        "   - softmax with one neuron $\\to$ constant 1 prediction always\n",
        "   \n",
        "We could have also used 2 neurons with categorical crossentropy and softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIkE_sIaX5Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we could set classes=1, but the default activation is softmax\n",
        "# softmax with one neuron is not the best idea...\n",
        "model = ResNet50(weights=None)\n",
        "pretrained_model = ResNet50(weights='imagenet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXAG5LQgX5Dz",
        "colab_type": "text"
      },
      "source": [
        "Removing the last layer and creating a new model which has 1 neuron at the end with sigmoid activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVjfT_8fX5D0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model._layers.pop()\n",
        "inputs = model.input\n",
        "output = model.layers[-1].output\n",
        "output = Dense(1, activation='sigmoid')(output)\n",
        "model = Model(inputs, output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4btQxIeX5D3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_model._layers.pop()\n",
        "inputs = pretrained_model.input\n",
        "output = pretrained_model.layers[-1].output\n",
        "output = Dense(1, activation='sigmoid')(output)\n",
        "pretrained_model = Model(inputs, output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5HH2qqSX5D7",
        "colab_type": "text"
      },
      "source": [
        "For the pre-trained model we freeze all the layers but the last."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv4FC-0SX5D8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in pretrained_model.layers[:-1]:\n",
        "    i.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgkR2v1VX5EA",
        "colab_type": "text"
      },
      "source": [
        "Check the models and compare the trainable parameters! Later it worth to chech the training times too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tl8SmzTX5EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6UA7HVaX5EE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfHu0ZYaX5EI",
        "colab_type": "text"
      },
      "source": [
        "## Dataloader\n",
        "\n",
        "Previously we just loaded all the data to memory and feeded the neural network with it. For larger datset it often happens that it simply does not fit into the RAM. Datagenerators are functions that provide one batch of data at a time.\n",
        "\n",
        "Tf-keras has a built-in ImageDataGenerator, we will use that here, but it does not take too much effort to write and own dataloader. Image augmentation can be done within the dataloader.\n",
        "\n",
        "The categories are matched from the folder names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjOUMGSDX5EJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imagenet_convert(img):\n",
        "    img  = img.astype(float)[...,::-1] # RGB --> BGR\n",
        "    img -= [103.939, 116.779, 123.68]\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR3gRNu0X5EM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagenerator = ImageDataGenerator(preprocessing_function=imagenet_convert)\n",
        "test_datagenerator  = ImageDataGenerator(preprocessing_function=imagenet_convert)\n",
        "\n",
        "train_datagenerator = train_datagenerator.flow_from_directory(\n",
        "        'train',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=16,\n",
        "        class_mode='binary')\n",
        "\n",
        "test_datagenerator = test_datagenerator.flow_from_directory(\n",
        "        'test',\n",
        "        target_size=(224, 224),\n",
        "        batch_size=16,\n",
        "        class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x-vUDoJX5EP",
        "colab_type": "text"
      },
      "source": [
        "Compile the models with Adam optimizer using learning rate of $10^{-4}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW9fqREfX5EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=Adam(lr=1e-4),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "pretrained_model.compile(optimizer=Adam(lr=1e-4),loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7-ppn3rX5EW",
        "colab_type": "text"
      },
      "source": [
        "### Fit the models for 25 epochs and run the validation efter every 5th epoch. \n",
        "\n",
        "The training time is pretty low for a single epoch, however validation is much slower. It happens because we have 40x images to validate on as to train on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "sqpJk4niX5EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(train_datagenerator, \n",
        "          validation_data=test_datagenerator, \n",
        "          validation_freq=5, epochs=25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4wLyef-X5Eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_model.fit(train_datagenerator, \n",
        "                     validation_data=test_datagenerator, \n",
        "                     validation_freq=5, epochs=25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQGv6LgVX5Ee",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "Both network became better by time, but the randomly initialized network could achieve only <70% (with some ideas we could surely get much better) accuracy while the pre-trained model achieved >97% accuracy.   \n",
        "Also, the training time was ~50% lower for the pre-trained model for each epoch.\n",
        "\n",
        "https://www.kaggle.com/c/dogs-vs-cats/leaderboard   \n",
        "6 years ago the winner achieved 98.9%, 10th place 97.9%, 30th place was 96.7% (but on a different test set)\n",
        "\n",
        "**We achieved those results with training for ~3 minutes (not counting the validation time).**\n",
        "\n",
        "\n",
        "### Further improvements\n",
        " - clean dataset, remove mislabeled images\n",
        " - augmentation\n",
        " - test time augmentation\n",
        " - careful learning rate schedule\n",
        " - fine-tuning other pre-trained models from the [model zoo](https://www.tensorflow.org/api_docs/python/tf/keras/applications/) and averaging them (ensemble)"
      ]
    }
  ]
}