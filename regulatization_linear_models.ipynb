{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regulatization_linear_models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPNYELjFDGcraQKQy8iTy1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patbaa/demo_notebooks/blob/master/regulatization_linear_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8fN5N9IdCu4",
        "colab_type": "text"
      },
      "source": [
        "# Regularization for linear models\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbbKFwZkQFk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBQKGggtdKV5",
        "colab_type": "text"
      },
      "source": [
        "Generate data with quadratic relationship plus added noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T1HWvBMIQEwc",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "np.random.seed(42)\n",
        "X = np.linspace(-1, 1, 20)\n",
        "features = PolynomialFeatures(degree=15).fit_transform(X[..., None])\n",
        "y = 20*X**2 + np.random.random(20)*7\n",
        "\n",
        "plt.plot(np.linspace(-1, 1, 100), \n",
        "         20*np.linspace(-1, 1, 100)**2 + 3.5, lw=5, label = 'truth', ls='--')\n",
        "plt.scatter(X, y, s=200, c = 'k')\n",
        "\n",
        "plt.xlim(-1.01, 1.01)\n",
        "#plt.axis('off')\n",
        "plt.legend(fontsize=15)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMisQ04KdUug",
        "colab_type": "text"
      },
      "source": [
        "Fitting a 15th order polynomial with Linear Regression results severe overfitting. Polynomial fitting can be done with linear regression after transforming the original $x$ variable to a feature vector that contains $[1, x, x^2, x^3, ..., x^{15}]$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2YwsCjZRGqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5KNxX6jQzIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr    = LinearRegression()\n",
        "lasso = Lasso()\n",
        "ridge = Ridge()\n",
        "\n",
        "lr.fit(features, y)\n",
        "lasso.fit(features, y)\n",
        "ridge.fit(features, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-w2WtPhQCen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "XX = np.linspace(-1, 1, 100)\n",
        "XX_features = PolynomialFeatures(degree=15).fit_transform(XX[..., None])\n",
        "\n",
        "plt.plot(XX, 20*XX**2 + 3.5, lw=5, label = 'truth', ls='--')\n",
        "plt.scatter(X, y, s=200, c = 'k')\n",
        "plt.plot(XX,  lr.predict(XX_features), lw=5, label = 'linear regression', ls='--')\n",
        "\n",
        "\n",
        "plt.xlim(-1.01, 1.01)\n",
        "#plt.axis('off')\n",
        "plt.legend(fontsize=15)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7HEwlbndzP_",
        "colab_type": "text"
      },
      "source": [
        "## Coefficients for the model: linear regression large plus/minus alternating\n",
        "\n",
        "Linear regression loss function \n",
        "$$ L = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y_i})^2$$\n",
        "\n",
        "Idea: punish the large coefficients!\n",
        " $$ L = L_{error} + L_{penalty}$$\n",
        "L1 regularization, Lasso:\n",
        " $$ L = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y_i})^2 + \\alpha \\sum_j |w_j|$$\n",
        "\n",
        "L2 regularization, Ridge:\n",
        " $$ L = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y_i})^2 + \\alpha \\sum_j |w_j|^2$$\n",
        "\n",
        "L1 and L2 combined: Elastic net\n",
        "\n",
        "$\\alpha$ is the regularization strength\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJq-g-svexdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "XX = np.linspace(-1, 1, 100)\n",
        "XX_features = PolynomialFeatures(degree=15).fit_transform(XX[..., None])\n",
        "\n",
        "plt.plot(XX, 20*XX**2 + 3.5, lw=5, label = 'truth', ls='--')\n",
        "plt.scatter(X, y, s=200, c = 'k')\n",
        "plt.plot(XX,  lr.predict(XX_features), lw=5, label = 'linear regression', ls='--')\n",
        "plt.plot(XX,  lasso.predict(XX_features), lw=5, label = 'lasso', ls='--')\n",
        "plt.plot(XX,  ridge.predict(XX_features), lw=5, label = 'ridge', ls='--')\n",
        "\n",
        "\n",
        "plt.xlim(-1.01, 1.01)\n",
        "#plt.axis('off')\n",
        "plt.legend(fontsize=15)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n5V1aW0QXT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ridge.coef_, 'o', label='ridge coefficients')\n",
        "plt.plot(lasso.coef_, 'o', label='lasso coefficients')\n",
        "plt.plot(lr.coef_, 'o', label='lr coefficients [log2]')\n",
        "plt.legend(fontsize=15)\n",
        "plt.show()\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ridge.coef_, 'o', label='ridge coefficients')\n",
        "plt.plot(lasso.coef_, 'o', label='lasso coefficients')\n",
        "plt.legend(fontsize=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pXsOOGlTwXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r_coef = []\n",
        "l_coef = []\n",
        "for r in np.linspace(0.05, 2.5, 100):\n",
        "    ridge = Ridge(alpha=r)\n",
        "    ridge.fit(features, y)\n",
        "    r_coef.append(ridge.coef_)\n",
        "\n",
        "    lasso = Lasso(alpha=r)\n",
        "    lasso.fit(features, y)\n",
        "    l_coef.append(lasso.coef_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mL9aO3kUCBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(18, 4))\n",
        "plt.plot(np.linspace(0.05, 2.5, 100), r_coef)\n",
        "plt.ylim(-7, 18)\n",
        "plt.title('Ridge, L2', fontsize=20)\n",
        "plt.ylabel('coefficients', fontsize=20)\n",
        "plt.xlabel('regularization strength', fontsize=20)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(18, 4))\n",
        "plt.title('Lasso, L1', fontsize=20)\n",
        "plt.ylim(-7, 18)\n",
        "plt.plot(np.linspace(0.05, 2.5, 100), l_coef)\n",
        "plt.ylabel('coefficients', fontsize=20)\n",
        "plt.xlabel('regularization strength', fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47KBZsUZb6rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.argmax(abs(l_coef[-20]))\n",
        "# feature selection --> max is the 2nd power"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6b6kv_cWEP5",
        "colab_type": "text"
      },
      "source": [
        "# Conclusions\n",
        " - L1 regularization prefers sparse coefficients\n",
        " - L2 is usually dense\n",
        " - overfitting is manageable with regularization\n",
        "   - complex models can be trained with many parameters\n",
        "\n",
        "\n",
        "Regularization is a general idea, there are regularization techniques for many other models too!"
      ]
    }
  ]
}